{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "635240c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch>=2.2.0 (from -r requirements.txt (line 1))\n",
      "  Using cached torch-2.9.1-cp312-cp312-win_amd64.whl.metadata (30 kB)\n",
      "Collecting transformers==4.30.2 (from -r requirements.txt (line 2))\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "Collecting pandas==2.0.3 (from -r requirements.txt (line 3))\n",
      "  Using cached pandas-2.0.3.tar.gz (5.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: still running...\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy==1.24.3 (from -r requirements.txt (line 4))\n",
      "  Using cached numpy-1.24.3.tar.gz (10.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (D:\\Anaconda\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"D:\\Anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 389, in <module>\n",
      "          main()\n",
      "        File \"D:\\Anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 373, in main\n",
      "          json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"D:\\Anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "          backend = _build_backend()\n",
      "                    ^^^^^^^^^^^^^^^^\n",
      "        File \"D:\\Anaconda\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 70, in _build_backend\n",
      "          obj = import_module(mod_path)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"D:\\Anaconda\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "          return _bootstrap._gcd_import(name[level:], package, level)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "        File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "        File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "        File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "        File \"<frozen importlib._bootstrap_external>\", line 995, in exec_module\n",
      "        File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-yrvbdemv\\overlay\\Lib\\site-packages\\setuptools\\__init__.py\", line 16, in <module>\n",
      "          import setuptools.version\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-yrvbdemv\\overlay\\Lib\\site-packages\\setuptools\\version.py\", line 1, in <module>\n",
      "          import pkg_resources\n",
      "        File \"C:\\Users\\HP\\AppData\\Local\\Temp\\pip-build-env-yrvbdemv\\overlay\\Lib\\site-packages\\pkg_resources\\__init__.py\", line 2172, in <module>\n",
      "          register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "                          ^^^^^^^^^^^^^^^^^^^\n",
      "      AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "ERROR: Failed to build 'numpy' when getting requirements to build wheel\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ffcc2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mNotebook 3: Model Training dengan IndoBERT\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mOptimized training untuk deteksi cyberbullying\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# IMPORT LIBRARIES\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# =====================================================\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook 3: Model Training dengan IndoBERT\n",
    "Optimized training untuk deteksi cyberbullying\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# IMPORT LIBRARIES\n",
    "# =====================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# SETUP DEVICE\n",
    "# =====================================================\n",
    "print(\"=== SETUP DEVICE ===\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "# =====================================================\n",
    "# HYPERPARAMETERS (OPTIMIZED)\n",
    "# =====================================================\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_RATIO = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "# =====================================================\n",
    "# LOAD DATA\n",
    "# =====================================================\n",
    "print(\"\\n=== LOADING DATA ===\")\n",
    "train_df = pd.read_csv('../data/processed/train.csv')\n",
    "val_df   = pd.read_csv('../data/processed/val.csv')\n",
    "test_df  = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)}\")\n",
    "print(f\"Val  : {len(val_df)}\")\n",
    "print(f\"Test : {len(test_df)}\")\n",
    "\n",
    "# =====================================================\n",
    "# CLASS WEIGHT (IMPORTANT)\n",
    "# =====================================================\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# =====================================================\n",
    "# DATASET CLASS\n",
    "# =====================================================\n",
    "class CyberbullyingDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff0eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING INDOBERT ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ IndoBERT loaded\n",
      "Train batches: 66\n",
      "Val batches  : 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================================================\n",
    "# LOAD TOKENIZER & MODEL\n",
    "# =====================================================\n",
    "print(\"\\n=== LOADING INDOBERT ===\")\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p1\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"✓ IndoBERT loaded\")\n",
    "\n",
    "# =====================================================\n",
    "# DATALOADERS\n",
    "# =====================================================\n",
    "train_dataset = CyberbullyingDataset(\n",
    "    train_df['cleaned_comment'].values,\n",
    "    train_df['label'].values,\n",
    "    tokenizer,\n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "val_dataset = CyberbullyingDataset(\n",
    "    val_df['cleaned_comment'].values,\n",
    "    val_df['label'].values,\n",
    "    tokenizer,\n",
    "    MAX_LEN\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches  : {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1049d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# OPTIMIZER & SCHEDULER\n",
    "# =====================================================\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# =====================================================\n",
    "# TRAIN & EVAL FUNCTIONS\n",
    "# =====================================================\n",
    "def train_epoch(model, data_loader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        correct += torch.sum(preds == labels)\n",
    "        total += labels.size(0)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    return correct.double() / total, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds_all = []\n",
    "    labels_all = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            correct += torch.sum(preds == labels)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            preds_all.extend(preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct.double() / total\n",
    "    f1 = f1_score(labels_all, preds_all, average='weighted')\n",
    "\n",
    "    return acc.item(), f1, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55961e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== START TRAINING ===\n",
      "\n",
      "--- Epoch 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66/66 [23:46<00:00, 21.62s/it]\n",
      "Validation: 100%|██████████| 15/15 [01:12<00:00,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6797 | Train Acc: 0.5782\n",
      "Val   Loss: 0.6413 | Val Acc: 0.5841 | Val F1: 0.5800\n",
      "✓ Best model saved (Val F1 = 0.5800)\n",
      "\n",
      "--- Epoch 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 66/66 [28:54<00:00, 26.28s/it]\n",
      "Validation: 100%|██████████| 15/15 [01:41<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5135 | Train Acc: 0.7517\n",
      "Val   Loss: 0.6692 | Val Acc: 0.6814 | Val F1: 0.6788\n",
      "✓ Best model saved (Val F1 = 0.6788)\n",
      "\n",
      "--- Epoch 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 18/66 [09:51<28:51, 36.08s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# TRAINING LOOP WITH EARLY STOPPING\n",
    "# =====================================================\n",
    "print(\"\\n=== START TRAINING ===\")\n",
    "\n",
    "history = {\n",
    "    'train_acc': [],\n",
    "    'train_loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_f1': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "best_val_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "\n",
    "    train_acc, train_loss = train_epoch(model, train_loader)\n",
    "    val_acc, val_f1, val_loss = eval_model(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    history['train_acc'].append(train_acc.item())\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), '../models/best_model.pt')\n",
    "        patience_counter = 0\n",
    "        print(f\"✓ Best model saved (Val F1 = {val_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"EarlyStopping {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(\"⛔ Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(\"\\n=== TRAINING FINISHED ===\")\n",
    "print(f\"Best Validation F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# SAVE TRAINING HISTORY\n",
    "# =====================================================\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv('../results/metrics/training_history.csv', index=False)\n",
    "print(\"✓ Training history saved\")\n",
    "\n",
    "# =====================================================\n",
    "# PLOT TRAINING HISTORY\n",
    "# =====================================================\n",
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history['train_acc'], label='Train Acc')\n",
    "plt.plot(history['val_acc'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/training_history.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n➡️ Lanjut ke Notebook 04: Evaluation Metrics\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
